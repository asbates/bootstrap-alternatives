---
title: "When the Bootstrap Fails and How to Fix It"
author: "Andrew Bates"
output: html_document
bibliography: FinalProject.bib
---

### Introduction

In this paper, we examine alternative resampling methods for constructing confidence intervals in two instances where the bootstrap is known to fail: the mean of a distribution with infinite variance and the maximum of a uniform distribution. For each example, we perform simulations to obtain an empirical coverage probability utilizing appropriate resampling methods: the $m$ out of $n$ bootstrap and subsampling. Several resample sizes are used to investigate the effect on the empirical coverage probability.


The bootstrap is a powerful and widely applicable statistical analysis tool, however, it does have limitations. There are many well known examples of when the bootstrap fails and various alternative methods to remedy this. For simplicity, we only consider two examples of bootstrap failure and two alternate methods for rectifying these failures. Additionally, we do not go into the mathematical details of why the bootstrap fails or why the alternative resampling methods are valid. Instead, we investigate the three methods through simulations.

### Examples of When the Bootstrap Fails

Suppose we have data $X_1, X_2, \ldots, X_n$ i.i.d. from some distribution $F$ and are interested in constructing a confidence interval for a parameter of interest $\theta$. If $\theta = E[X_1]$ and Var$[X_1] < \infty$, the standard approach is to invoke the central limit theorem and construct confidence intervals based on the normal distribution. An alternative to this approach is to use the bootstrap, which is valid in this case. But what if the variance is infinite? Then the central limit theorem is not applicable and, as Athreya shows, neither is the bootstrap [-@infmean]. 

The second example we consider is when $\theta$ is an extreme order statistic. For example, $X_1, X_2, \ldots, X_n$ are from a uniform distribution on $(0,\theta)$. Again, in this instance the bootstrap is unsuitable [@cis;@intro] so in order to construct a confidence interval, we need an alternative method.


### Remedies

With the bootstrap, we draw a sample of size $n$ from our data with replacement obtaining a bootstrap sample. Then we compute $\hat{\theta}$, an estimate of $\theta$. After repeating this a large number of times, we have many $\hat{\theta}$'s and can use these values to obtain an approximate confidence interval for $\theta$.

Two alternative resampling schemes, which are valid in the examples we consider, are the $m$ out of $n$ bootstrap and subsampling. The $m$ out of $n$ bootstrap is a generalization of the original bootstrap. The difference being that instead of drawing a sample of size $n$ from the data, we draw a sample of size $m$ with $m < n$. Subsampling takes a similar approach in that samples of size $b$, where $b < n$, are drawn from the data. However, with subsampling, resamples are obtained by sampling without replacement as opposed to sampling with replacement in the bootstrap. This is a general approach to resampling which is valid under very few assumptions [@large]. The choice of resample size $m$ (or $b$) is essential in these methods [@choosem;@large]. This is not an easy problem though. As this is an empirical study, rather than trying to determine the optimal choice of $m$, we simply construct confidence intervals for several values.


### Example 1: $X_1, \ldots, X_n \sim t_2$

In this example our data is i.i.d from a $t$ distribution with $2$ degrees of freedom. Our parameter of interest is the mean and our statistic is the sample mean. In this case $\theta = E[X_1] = 0$ and Var$[X_1] = \infty$. We used sample sizes of $n = 100, 200, 500, 1000$ and our resample sizes were chosen to be $b = n^{1/6}, n^{1/3},n^{1/2},n^{2/3},n^{5/6}$. For each combination of $n$ and $b$, we computed $1,000$ $95\%$ confidence intervals using both the $m$ out of $n$ bootstrap and subsampling. The empirical coverage probabilities are given in tables $1$ and $2$. In addition to the empirical coverage, we computed the average interval lengths for both methods which are given in tables $3$ and $4$.


\begin{table}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
& $b = n^{1/6}$ & $n^{1/3}$ &  $n^{1/2}$ & $n^{2/3}$ & $n^{5/6}$ \\
\hline
\hline
$n = 100$ & 0.962 & 0.989 & 0.977 & 0.964 & 0.963 \\
\hline
200 & 0.924 & 0.973 & 0.977 & 0.971 & 0.974 \\
\hline
500 & 0.892 & 0.940 & 0.986 & 0.974 & 0.972 \\
\hline
1000 & 0.887 & 0.924 & 0.968 & 0.980 & 0.979 \\
\hline
\end{tabular}
\label{table:ex1_boot_cov}
\caption{Coverage using $m$ out of $n$ bootstrap for $X_i \sim t_2$}
\end{center}
\end{table}


\begin{table}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
& $b = n^{1/6}$ & $n^{1/3}$ &  $n^{1/2}$ & $n^{2/3}$ & $n^{5/6}$ \\
\hline
\hline
$n = 100$ & 0.951 & 0.988 & 0.964 & 0.939 & 0.824 \\
\hline
200 & 0.921 & 0.963 & 0.982 & 0.948 & 0.853 \\
\hline
500 & 0.899 & 0.947 & 0.975 & 0.962 & 0.892 \\
\hline
1000 & 0.884 & 0.924 & 0.966 & 0.966 & 0.928 \\
\hline
\end{tabular}
\label{table:ex1_sub_cov}
\caption{Coverage using subsampling for $X_i \sim t_2$}
\end{center}
\end{table}

For the $m$ out of $n$ bootstrap, the resample size of $b = n^{1/6}$ seems to be the poorest choice as the empirical coverages are furthest from $95\%$. Also, a larger sample size does not seem to offer any improvement for this value of $b$. Indeed, as $n$ grows, the empirical coverage gets further away from the desired coverage for both methods. For subsampling, $b = n^{5/6}$ gave the worst coverage although better coverage was obtained for larger $n$. For $b = n^{1/3}$, both methods give similar coverage results but subsampling outperforms the $m$ out of $n$ bootstrap in terms of interval length. 

\begin{table}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
& $b = n^{1/6}$ & $n^{1/3}$ &  $n^{1/2}$ & $n^{2/3}$ & $n^{5/6}$ \\
\hline
\hline
$n = 100$ & 1.004 & 1.460 & 1.053 & 1.013 & 1.050 \\
\hline
200 & 0.698 & 0.872 & 0.876 & 0.779 & 0.786 \\
\hline
500 & 0.417 & 0.485 & 0.628 & 0.532 & 0.630 \\
\hline
1000 & 0.305 & 0.339 & 0.426 & 0.385 & 0.430 \\
\hline
\end{tabular}
\label{table:ex1_boot_len}
\caption{Average length using $m$ out of $n$ bootstrap for $X_i \sim t_2$}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
& $b = n^{1/6}$ & $n^{1/3}$ &  $n^{1/2}$ & $n^{2/3}$ & $n^{5/6}$ \\
\hline
\hline
$n = 100$ & 1.021 & 1.290 & 1.002 & 0.932 & 0.718 \\
\hline
200 & 0.690 & 0.854 & 0.811 & 0.702 & 0.564 \\
\hline
500 & 0.419 & 0.480 & 0.585 & 0.474 & 0.397 \\
\hline
1000 & 0.306 & 0.338 & 0.432 & 0.392 & 0.315 \\
\hline
\end{tabular}
\label{table:ex1_sub_len}
\caption{Average length using subsampling for $X_i \sim t_2$}
\end{center}
\end{table}

Actually, subsampling tended to produce shorter confidence intervals. To visualize this, we produced a plot of average coverage length vs. resample size which is given in Figure $1$. This plot is for $n = 1000$ but the other sample sizes gave a similar result. We can see that for the smaller values of $b$, the interval lengths are about the same but the subsampling intervals improve with larger $b$. It should be noted, however, that for $b = n^{5/6}$ subsampling produced significantly shorter intervals but had poorer coverage compared to the $m$ out of $n$ bootstrap. It seems as though subsampling with $b = n^{2/3}$ is the best choice in this situation as the coverage is closer to $95\%$ and the average interval length is lower than with the $m$ out of $n$ bootstrap.  

![Average interval length for $X_i \sim t_2$ and $n = 1000$](lengthplot-t.jpeg){width=65%}


One interesting thing to note is that the coverage for the bootstrap was (in increasing order by $n$) $0.971, 0.967, 0.967, 0.96$ which is not too far off from the desired coverage. This is misleading though because the bootstrap is not appropriate in this example and so we should not let the simulated coverage probability give us a false sense of hope.
 
 

 
### Example 2: $X_1, \ldots, X_n \sim U(0,10)$

In this example our data is i.i.d from a uniform distribution on the interval $(0,10)$. Our parameter of interest is the upper endpoint of the interval and our statistic is $\hat{\theta} = \max(X_1, \ldots, X_n)$. This example is essentially the same as Example $1$ but with different data. The same values ($n$,$b$, etc.) were used throughout. Tables $5$ and $6$ give the empirical coverage probabilities and Tables $7$ and $8$ give the average length of the intervals.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
& $b = n^{1/6}$ & $n^{1/3}$ &  $n^{1/2}$ & $n^{2/3}$ & $n^{5/6}$ \\
\hline
\hline
$n = 100$ & 0.785 & 0.902 & 0.935 & 0.951 & 0.937 \\
\hline
200 & 0.772 & 0.925 & 0.963 & 0.954 & 0.943 \\
\hline
500 & 0.794 & 0.941 & 0.961 & 0.966 & 0.949 \\
\hline
1000 & 0.846 & 0.924 & 0.953 & 0.969 & 0.957 \\
\hline
\end{tabular}
\label{table:ex1_sub_len}
\caption{Coverage using $m$ out of $n$ bootstrap for $X_i \sim U(0,10)$}
\end{center}
\end{table}


\begin{table}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
& $b = n^{1/6}$ & $n^{1/3}$ &  $n^{1/2}$ & $n^{2/3}$ & $n^{5/6}$ \\
\hline
\hline
$n = 100$ & 0.797 & 0.902 & 0.944 & 0.941 & 0.845 \\
\hline
200 & 0.782 & 0.904 & 0.948 & 0.948 & 0.885 \\
\hline
500 & 0.797 & 0.912 & 0.966 & 0.952 & 0.899 \\
\hline
1000 & 0.862 & 0.919 & 0.963 & 0.957 & 0.924 \\
\hline
\end{tabular}
\label{table:ex1_sub_len}
\caption{Coverage using subsampling for $X_i \sim U(0,10)$}
\end{center}
\end{table}

\newpage



\begin{table}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
& $b = n^{1/6}$ & $n^{1/3}$ &  $n^{1/2}$ & $n^{2/3}$ & $n^{5/6}$ \\
\hline
\hline
$n = 100$ & 0.163 & 0.236 & 0.299 & 0.319 & 0.330 \\
\hline
200 & 0.082 & 0.128 & 0.159 & 0.170 & 0.169 \\
\hline
500 & 0.033 & 0.056 & 0.067 & 0.070 & 0.068 \\
\hline
1000 & 0.021 & 0.030 & 0.034 & 0.035 & 0.035 \\
\hline
\end{tabular}
\label{table:ex1_sub_len}
\caption{Average length using $m$ out of $n$ bootstrap for $X_i \sim U(0,10)$}
\end{center}
\end{table}


\begin{table}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
& $b = n^{1/6}$ & $n^{1/3}$ &  $n^{1/2}$ & $n^{2/3}$ & $n^{5/6}$ \\
\hline
\hline
$n = 100$ & 0.162 & 0.232 &  0.285 & 0.289 & 0.235 \\
\hline
200 & 0.082 & 0.126 & 0.152 & 0.155 & 0.127 \\
\hline
500 & 0.033 & 0.056 & 0.066 & 0.065 & 0.056 \\
\hline
1000 & 0.021 & 0.030 & 0.034 & 0.034 & 0.029 \\
\hline
\end{tabular}
\label{table:ex1_sub_len}
\caption{Average length using subsampling for $X_i \sim U(0,10)$}
\end{center}
\end{table}

The choice of $b = n^{1/6}$ results in the poorest coverage probability for both methods. In contrast to Example $1$, the results are a bit clearer as to which resample size produces the best coverage. For both the $m$ out of $n$ bootstrap and subsampling, $b = n^{2/3}$ gives coverage probability that is closest to $95\%$. $b = n^{1/2}$ seems to be the next best choice for both methods. As in Example $1$, subsampling with $b = n^{5/6}$ produced poorer coverage than the $m$ out of $n$ bootstrap.

For the smallest three values of $b$, the average interval length for both methods is essentially the same. However, for the larger values of $b$, subsampling again leads to shorter intervals. For this reason, our conclusion is the same as in Example $1$. Subsampling with $b = n^{2/3}$ seems to be the best option.


For this example, bootstrapping produced covererage probabilities of $0.882, 0.878, 0..881, 0.885$. This may seem like a good thing in the sense that several of the coverage probabilities for the alternate methods were worse. But, as was mentioned in Example $1$, this is misleading because the bootstrap is not appropriate in this situation so we should not put much stock in its results.


### Conclusion

Based on our simulations, it seems as though subsampling performs better than the $m$ out of $n$ bootstrap, at least in the specific examples considered, although either method would likely be suitable in most circumstances as they gave similar results. What is more relevant though, is that although the bootstrap is applicable in many situations, there are instances in which it is invalid. From a practical standpoint, the analyst should be aware of these instances and instead use suitable alternatives. Otherwise, they may be overly confident in an unsound result.


\newpage

### References


